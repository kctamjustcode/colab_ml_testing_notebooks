{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP/zCRAzjt7Xpa/ZQ+I349S"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"3XoukozQJubx"},"outputs":[],"source":["# lr, size, range, algo\n","\n","import torch, random\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","\n","class SimpleNN(nn.Module):\n","    def __init__(self):\n","        super(SimpleNN, self).__init__()\n","        self.fc1 = nn.Linear(2, 6)\n","        self.fc2 = nn.Linear(6, 1)\n","        self.relu = nn.ReLU()\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        x = self.relu(self.fc1(x))\n","        x = self.sigmoid(self.fc2(x))\n","        return x\n","\n","class SeqModel(nn.Module):\n","    def __init__(self, input_features, output_features, hidden_units=8):\n","      super(SeqModel, self).__init__()\n","      self.linear_layer_stack = nn.Sequential(\n","            nn.Linear(input_features, hidden_units),\n","            nn.ReLU(),\n","            nn.Linear(hidden_units, output_features),\n","        )\n","\n","    def forward(self, x):\n","      return self.linear_layer_stack(x)\n"]},{"cell_type":"code","source":["#X_train = torch.tensor([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]])\n","#y_train = torch.tensor([[0.0], [1.0], [1.0], [0.0]])\n","\n","X_trn = [[random.uniform(0, 1), random.uniform(0, 1)] for _ in range(3000)]\n","y_trn = [[round(X_trn[i][0])^round(X_trn[i][1])] for i in range(3000)]\n","X_train = torch.tensor(X_trn, dtype=torch.float32)\n","y_train = torch.tensor(y_trn, dtype=torch.float32)\n","print(X_train[:10], y_train[:10])\n","\n","# Instantiate the Model, Define Loss Function and Optimizer\n","model = SimpleNN()\n","#criterion = nn.MSELoss()\n","criterion = nn.BCEWithLogitsLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.02)\n","\n","for epoch in range(500):\n","    model.train()\n","\n","    # Forward pass\n","    outputs = model(X_train)\n","    #print(outputs)\n","    loss = criterion(outputs, y_train)\n","\n","    # Backward pass and optimize\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","    if (epoch + 1) % 100 == 0:\n","        print(f'Epoch [{epoch + 1}/500], Loss: {loss.item():.4f}')\n","\n","model.eval()\n","with torch.no_grad():\n","    test_data = torch.tensor([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]])\n","    predictions = model(test_data)\n","    print(f'Predictions:\\n{predictions}')\n","    # results very sensitive to random initial parameters\n","\n","\n","# Instantiate the Model, Define Loss Function and Optimizer\n","model_seq = SeqModel(2,1)\n","#criterion_seq = nn.MSELoss()\n","criterion_seq = nn.BCEWithLogitsLoss()\n","optimizer_seq = optim.Adam(model_seq.parameters(), lr=0.01)\n","\n","for epoch in range(500):\n","    model_seq.train()\n","\n","    # Forward pass\n","    outputs_seq = model_seq(X_train)\n","    loss_seq = criterion_seq(outputs_seq, y_train)\n","\n","    # Backward pass and optimize\n","    optimizer_seq.zero_grad()\n","    loss_seq.backward()\n","    optimizer_seq.step()\n","\n","    if (epoch + 1) % 100 == 0:\n","        print(f'Epoch [{epoch + 1}/500], Loss: {loss_seq.item():.4f}')\n","\n","model_seq.eval()\n","with torch.no_grad():\n","    test_data_seq = torch.tensor([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]])\n","    predictions_seq = model_seq(test_data_seq)\n","    print(f'Predictions:\\n{predictions_seq}')\n","    # results very sensitive to random initial parameters"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aFsRU-R95p3d","executionInfo":{"status":"ok","timestamp":1753269727747,"user_tz":-480,"elapsed":1241,"user":{"displayName":"TAM Work","userId":"15604394458332215355"}},"outputId":"9ead5de5-bc16-44c7-988e-1967d82b256a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.4147, 0.7176],\n","        [0.9306, 0.3451],\n","        [0.1739, 0.4533],\n","        [0.0716, 0.4903],\n","        [0.3511, 0.0151],\n","        [0.4660, 0.3103],\n","        [0.6940, 0.1621],\n","        [0.5373, 0.3260],\n","        [0.1764, 0.7444],\n","        [0.0380, 0.9287]]) tensor([[1.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [1.]])\n","Epoch [100/500], Loss: 0.6352\n","Epoch [200/500], Loss: 0.5883\n","Epoch [300/500], Loss: 0.5687\n","Epoch [400/500], Loss: 0.5318\n","Epoch [500/500], Loss: 0.5248\n","Predictions:\n","tensor([[1.2508e-09],\n","        [1.0000e+00],\n","        [1.0000e+00],\n","        [2.4890e-09]])\n","Epoch [100/500], Loss: 0.5159\n","Epoch [200/500], Loss: 0.3899\n","Epoch [300/500], Loss: 0.3755\n","Epoch [400/500], Loss: 0.3745\n","Epoch [500/500], Loss: 0.3744\n","Predictions:\n","tensor([[-3.4288],\n","        [ 7.4553],\n","        [ 7.4015],\n","        [-3.0542]])\n"]}]},{"cell_type":"code","source":["print(model(torch.tensor([[0.52,0.01]])))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":141},"id":"1QNSvJB3f9Ry","executionInfo":{"status":"error","timestamp":1753277808513,"user_tz":-480,"elapsed":234,"user":{"displayName":"TAM Work","userId":"15604394458332215355"}},"outputId":"7c57553c-69e7-4dbf-906d-e1cf21066406"},"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'model' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1-1915070585.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.55\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"]}]},{"cell_type":"code","source":["class Generator(nn.Module):\n","    def __init__(self, noise_dim):\n","        super(Generator, self).__init__()\n","        self.noise_dim = noise_dim\n","        self.main = nn.Sequential(\n","            nn.Linear(noise_dim, 4),\n","            nn.ReLU(True),\n","            nn.Linear(4, 2)\n","        )\n","\n","    def forward(self, x):\n","        return self.main(x)"],"metadata":{"id":"PdSkswbAJ2sf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Discriminator(nn.Module):\n","    def __init__(self):\n","        super(Discriminator, self).__init__()\n","        self.fc1 = nn.Linear(2, 16)\n","        self.fc2 = nn.Linear(16, 1)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        x = self.relu(self.fc1(x))\n","        x = self.fc2(x)\n","        return x"],"metadata":{"id":"pHGscjMpKFMg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["NOISE_DIM = 4\n","\n","generator = Generator(NOISE_DIM)\n","discriminator = Discriminator()"],"metadata":{"id":"DEwWbkMkKM3u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","generator = generator.to(device)\n","discriminator = discriminator.to(device)"],"metadata":{"id":"lxR7zonmKX8X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["criterion = nn.BCEWithLogitsLoss()\n","#criterion = nn.MSELoss()\n","\n","generator_optimizer = optim.Adam(generator.parameters(), lr=0.02, betas=(0.5, 0.999))\n","discriminator_optimizer = optim.Adam(discriminator.parameters(), lr=0.02, betas=(0.5, 0.999))\n","\n","NUM_EPOCHS = 200\n","BATCH_SIZE = 256\n","SAMPLE_SIZE = 10000\n","\n","x_trn = [[random.uniform(0, 1), random.uniform(0, 1)] for _ in range(SAMPLE_SIZE)]\n","y_trn = [round(x_trn[i][0])^round(x_trn[i][1]) for i in range(SAMPLE_SIZE)]\n","train_dataset =  [[x_trn[i][0],x_trn[i][1],y_trn[i]] for i in range(SAMPLE_SIZE)]\n","train_dataset = torch.tensor(train_dataset)\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)"],"metadata":{"id":"QcKjbKqFKYn2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# malfunctioning, attempting ideas: discriminator~basic dnn with gan attacked\n","\n","for epoch in range(NUM_EPOCHS):\n","    for test in train_loader:\n","\n","      real_images = test[:,:2]\n","      #real_images = real_images.to(device)\n","\n","      discriminator_optimizer.zero_grad()\n","      #real_labels = torch.ones(real_images.size(0), 1, device=device)\n","      real_labels = test[:,2:]\n","      #print(real_labels)\n","      real_outputs = discriminator(real_images)\n","      real_loss = criterion(real_outputs, real_labels)\n","      #real_loss.backward()\n","\n","      noise = torch.randn(real_images.size(0), NOISE_DIM, device=device)\n","      #print(noise)\n","      fake_images = generator(noise)  # extending domain to negative side in normality\n","      real_list = real_labels.tolist()\n","      fake_labels = torch.tensor([[round(real_list[i][0])^1] for i in range(len(real_list))], dtype=torch.float32)\n","      fake_outputs = discriminator(fake_images.detach())\n","      #print(fake_labels)\n","\n","      #fo_tl = fake_outputs.tolist()\n","      #fake_outputs_labels = torch.tensor([[round(fo_tl[i][0])] for i in range(len(fo_tl))], dtype=torch.float32)\n","      fake_loss = criterion(fake_outputs, fake_labels)\n","      #print(fake_outputs_labels)\n","      #fake_loss.backward()\n","      d_loss = (real_loss + fake_loss) / 2\n","      d_loss.backward()\n","      discriminator_optimizer.step()\n","\n","      generator_optimizer.zero_grad()\n","      #fake_labels = real_labels\n","      gen_images = generator(noise)\n","      gen_outputs = discriminator(gen_images)\n","      gen_loss = criterion(gen_outputs, real_labels)\n","      gen_loss.backward()\n","      generator_optimizer.step()\n","\n","    if epoch == 0 or epoch == NUM_EPOCHS-1:\n","      print(f'Epoch [{epoch+1}/{NUM_EPOCHS}]'#, Step [{epoch+1}/{len(train_loader)}], '\n","            f'Discriminator Loss: {d_loss.item():.4f}, '\n","            f'Generator Loss: {gen_loss.item():.4f}')\n","    elif epoch % 10 == 0 and epoch != 0:\n","      print(f'Epoch [{epoch}/{NUM_EPOCHS}]'#, Step [{epoch+1}/{len(train_loader)}], '\n","            f'Discriminator Loss: {d_loss.item():.4f}, '\n","            f'Generator Loss: {gen_loss.item():.4f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z0cFvvJuLX24","executionInfo":{"status":"ok","timestamp":1753269871466,"user_tz":-480,"elapsed":35701,"user":{"displayName":"TAM Work","userId":"15604394458332215355"}},"outputId":"5ec2a8ad-56ff-4a88-9e37-a5fb2697ba94"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/200]Discriminator Loss: 0.3648, Generator Loss: 0.6941\n","Epoch [10/200]Discriminator Loss: 0.3503, Generator Loss: 0.7814\n","Epoch [20/200]Discriminator Loss: 0.3697, Generator Loss: 0.9940\n","Epoch [30/200]Discriminator Loss: 0.3503, Generator Loss: 0.7466\n","Epoch [40/200]Discriminator Loss: 0.3507, Generator Loss: 0.8587\n","Epoch [50/200]Discriminator Loss: 0.3754, Generator Loss: 0.6942\n","Epoch [60/200]Discriminator Loss: 0.3643, Generator Loss: 0.9045\n","Epoch [70/200]Discriminator Loss: 0.3836, Generator Loss: 1.2288\n","Epoch [80/200]Discriminator Loss: 0.3563, Generator Loss: 0.6946\n","Epoch [90/200]Discriminator Loss: 0.3571, Generator Loss: 0.7379\n","Epoch [100/200]Discriminator Loss: 0.3229, Generator Loss: 1.4454\n","Epoch [110/200]Discriminator Loss: 0.3595, Generator Loss: 0.6940\n","Epoch [120/200]Discriminator Loss: 0.3728, Generator Loss: 0.7154\n","Epoch [130/200]Discriminator Loss: 0.3687, Generator Loss: 0.7189\n","Epoch [140/200]Discriminator Loss: 0.3472, Generator Loss: 0.8608\n","Epoch [150/200]Discriminator Loss: 0.3554, Generator Loss: 0.7452\n","Epoch [160/200]Discriminator Loss: 0.3858, Generator Loss: 0.6935\n","Epoch [170/200]Discriminator Loss: 0.3502, Generator Loss: 0.7483\n","Epoch [180/200]Discriminator Loss: 0.3451, Generator Loss: 0.9036\n","Epoch [190/200]Discriminator Loss: 0.3536, Generator Loss: 0.8996\n","Epoch [200/200]Discriminator Loss: 0.3727, Generator Loss: 0.9583\n"]}]},{"cell_type":"code","source":["def generate_and_save_images(model, dis_model, epoch, noise):\n","    model.eval()\n","    with torch.no_grad():\n","        fake_images = model(noise).cpu()\n","        print(noise)\n","        print(fake_images)\n","        print(dis_model(fake_images))\n","test_noise = torch.randn(4, NOISE_DIM, device=device)\n","generate_and_save_images(generator, discriminator, NUM_EPOCHS, test_noise)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gPQzCRgoRh--","executionInfo":{"status":"ok","timestamp":1753269875253,"user_tz":-480,"elapsed":42,"user":{"displayName":"TAM Work","userId":"15604394458332215355"}},"outputId":"54b141cc-3934-42df-ed27-cc7f403c5d3c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 1.3549, -0.6994,  1.0298,  0.5693],\n","        [-1.6062,  0.5869,  1.5516, -0.4347],\n","        [-0.4399, -0.9809, -0.5423,  0.8993],\n","        [ 2.8288,  0.5911,  0.2908, -1.4706]])\n","tensor([[ 1.1218, -3.7041],\n","        [ 1.1218, -3.7041],\n","        [ 1.1218, -3.7041],\n","        [ 1.1218, -3.7041]])\n","tensor([[0.8984],\n","        [0.8984],\n","        [0.8984],\n","        [0.8984]])\n"]}]},{"cell_type":"code","source":["print(generator(torch.randn(1, NOISE_DIM, device=device)))\n","print(discriminator(torch.tensor([[0.83,0.13]])))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d20yZOFzrOED","executionInfo":{"status":"ok","timestamp":1753269913257,"user_tz":-480,"elapsed":57,"user":{"displayName":"TAM Work","userId":"15604394458332215355"}},"outputId":"8cf31860-b9a7-466f-a49d-4d3c4f874d1c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 1.1218, -3.7041]], grad_fn=<AddmmBackward0>)\n","tensor([[-67.9189]], grad_fn=<AddmmBackward0>)\n"]}]},{"cell_type":"code","source":["# theoretical gan: simply generate images and making the discriminator considering fake as real\n","\n","for epoch in range(NUM_EPOCHS):\n","    for test in train_loader:\n","\n","      real_signal = test[:,:2]\n","      #real_images = real_images.to(device)\n","\n","      real_labels = torch.ones(real_signal.size(0), 1, device=device)\n","      fake_labels = torch.zeros(real_signal.size(0), 1, device=device)\n","\n","      discriminator_optimizer.zero_grad()\n","      noise = torch.randn(real_signal.size(0), NOISE_DIM, device=device)\n","\n","      #real_labels = test[:,2:]\n","      #print(real_labels)\n","      real_outputs = discriminator(real_signal)\n","      real_loss = criterion(real_outputs, real_labels)\n","      #real_loss.backward()\n","\n","      #print(noise)\n","      fake_signal = generator(noise)\n","      #real_list = real_labels.tolist()\n","      #fake_labels = torch.tensor([[round(real_list[i][0])^1] for i in range(len(real_list))], dtype=torch.float32)\n","      fake_outputs = discriminator(fake_signal)\n","      #print(fake_labels)\n","\n","      #fo_tl = fake_outputs.tolist()\n","      #fake_outputs_labels = torch.tensor([[round(fo_tl[i][0])] for i in range(len(fo_tl))], dtype=torch.float32)\n","      fake_loss = criterion(fake_outputs, fake_labels)\n","      #print(fake_outputs_labels)\n","      #fake_loss.backward()\n","      d_loss = (real_loss + fake_loss) / 2\n","      d_loss.backward()\n","      discriminator_optimizer.step()\n","\n","      generator_optimizer.zero_grad()\n","      gen_signal = generator(noise)\n","      gen_outputs = discriminator(gen_signal)\n","      gen_loss = criterion(gen_outputs, real_labels)\n","      gen_loss.backward()\n","      generator_optimizer.step()\n","\n","    if epoch == 0 or epoch == NUM_EPOCHS-1:\n","      print(f'Epoch [{epoch+1}/{NUM_EPOCHS}]'#, Step [{epoch+1}/{len(train_loader)}], '\n","            f'Discriminator Loss: {d_loss.item():.4f}, '\n","            f'Generator Loss: {gen_loss.item():.4f}')\n","    elif epoch % 10 == 0 and epoch != 0:\n","      print(f'Epoch [{epoch}/{NUM_EPOCHS}]'#, Step [{epoch+1}/{len(train_loader)}], '\n","            f'Discriminator Loss: {d_loss.item():.4f}, '\n","            f'Generator Loss: {gen_loss.item():.4f}')"],"metadata":{"id":"HGWJ-MK2cINa","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4a7f842d-db5e-469e-de00-92db6a8bcdef"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/200]Discriminator Loss: 4.6938, Generator Loss: 8.4557\n","Epoch [10/200]Discriminator Loss: 0.6689, Generator Loss: 0.7763\n","Epoch [20/200]Discriminator Loss: 0.4514, Generator Loss: 0.9949\n","Epoch [30/200]Discriminator Loss: 0.7657, Generator Loss: 0.7656\n","Epoch [40/200]Discriminator Loss: 0.4281, Generator Loss: 1.3071\n","Epoch [50/200]Discriminator Loss: 0.2693, Generator Loss: 1.7138\n","Epoch [60/200]Discriminator Loss: 0.6284, Generator Loss: 0.8359\n","Epoch [70/200]Discriminator Loss: 0.4112, Generator Loss: 1.3021\n","Epoch [80/200]Discriminator Loss: 0.2332, Generator Loss: 1.5395\n","Epoch [90/200]Discriminator Loss: 0.3241, Generator Loss: 1.7197\n","Epoch [100/200]Discriminator Loss: 0.4626, Generator Loss: 0.9129\n","Epoch [110/200]Discriminator Loss: 0.6193, Generator Loss: 1.2024\n","Epoch [120/200]Discriminator Loss: 0.5965, Generator Loss: 1.1177\n"]}]},{"cell_type":"code","source":["def generate_and_save_images(model, dis_model, epoch, noise):\n","    model.eval()\n","    with torch.no_grad():\n","        fake_images = model(noise).cpu()\n","        print(noise)\n","        print(fake_images)\n","        print(dis_model(fake_images))\n","test_noise = torch.randn(4, NOISE_DIM, device=device)\n","generate_and_save_images(generator, discriminator, NUM_EPOCHS, test_noise)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9FSKZtvB_GOM","executionInfo":{"status":"ok","timestamp":1753269561020,"user_tz":-480,"elapsed":40,"user":{"displayName":"TAM Work","userId":"15604394458332215355"}},"outputId":"a1b91834-9975-4e35-9ee2-19f6d5c0e9b6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 0.7422, -0.0878, -1.3394, -1.7695],\n","        [ 0.4495,  2.0385,  0.2653, -0.2018],\n","        [-0.0385, -0.0175,  1.9948, -0.2122],\n","        [ 0.2330,  0.5247,  0.2079, -0.4859]])\n","tensor([[0.3880, 0.3926],\n","        [0.3880, 0.3926],\n","        [0.3880, 0.3926],\n","        [0.3880, 0.3926]])\n","tensor([[0.3112],\n","        [0.3112],\n","        [0.3112],\n","        [0.3112]])\n"]}]},{"cell_type":"code","source":["print(model(torch.tensor([[0.0,0.8]])))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OJJ3nezdAqQo","executionInfo":{"status":"ok","timestamp":1753269616860,"user_tz":-480,"elapsed":21,"user":{"displayName":"TAM Work","userId":"15604394458332215355"}},"outputId":"89a23e53-0cc4-4dc4-983a-28982ebb8bac"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.9768]], grad_fn=<SigmoidBackward0>)\n"]}]}]}